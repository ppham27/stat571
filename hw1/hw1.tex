\documentclass[11pt, letterpaper]{article}
\setlength{\parindent}{0in}
\setlength{\textheight}{8.7in}
\setlength{\textwidth}{6.8in}
\setlength{\oddsidemargin}{-0.3in}
\setlength{\evensidemargin}{0.0in}
\addtolength{\topmargin}{-1in}
\setlength{\parskip}{0.1in}

\usepackage{amsmath, amsfonts, color}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{pdfpages}
\newcommand*{\justifyheading}{\raggedleft}


\renewcommand{\baselinestretch}{1.0}

\newcommand{\bx}{{\bm x}}
\newcommand{\bX}{{\bm X}}
\newcommand{\by}{{\bm y}}
\newcommand{\bY}{{\bm Y}}
\newcommand{\bW}{{\bm W}}
\newcommand{\bG}{{\bm G}}
\newcommand{\bR}{{\bm R}}
\newcommand{\bZ}{{\bm Z}}
\newcommand{\bV}{{\bm V}}
\newcommand{\bL}{{\bm L}}
\newcommand{\bz}{{\bm z}}
\newcommand{\be}{{\bm e}}
\newcommand{\bgamma}{{\bm \gamma}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\balpha}{{\bm \alpha}}
\newcommand{\bSigma}{{\bm \Sigma}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\btheta}{{\bm \theta}}
\newcommand{\bepsilon}{{\bm \epsilon}}
\newcommand{\bone}{{\bm 1}}
\newcommand{\bzero}{{\bm 0}}
\newcommand{\bC}{{\bm C}}
\newcommand{\bI}{{\bm I}}
\newcommand{\bA}{{\bm A}}
\newcommand{\bB}{{\bm B}}
\newcommand{\bQ}{{\bm Q}}
\newcommand{\bS}{{\bm S}}
\newcommand{\bD}{{\bm D}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cL}{\mathcal{L}}

\newcommand{\beas}{\begin{eqnarray*}}
\newcommand{\eeas}{\end{eqnarray*}}

\newenvironment{equationarrayright}{
                          \begin{eqnarray*}
                          \begin{array}{rcll}
                         }{
                          \end{array}
                          \end{eqnarray*}
                         }
\newcommand{\bear}{\begin{equationarrayright}}
\newcommand{\eear}{\end{equationarrayright}}

\renewcommand\arraystretch{1.3}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{STAT/BIOST 571: Homework 1}
\date{\today}
\author{Philip Pham}

\begin{document}

\maketitle

\section*{Problem 1: Non-parametric linear regression (5 points)}
{In the context of random $\bX$ linear regression, consider $n$ independent observations $(x_i, Y_i)$ generated as follows.  
First, the indepenent variable $x_i$ is selected according to 
\[
x_i = \left\{ \begin{array}{cl} 0 & \textrm{w.p. 1/3} \\  
                                          1 & \textrm{w.p. 1/3} \\ 
                                          3 & \textrm{w.p. 1/3} \end{array} \right. ,
\]
then the systematic component of $Y_i$ is set to
\[
\mu_i = \left\{ \begin{array}{cl} 0.6 & \textrm{if } x_i = 0 \\  
                                           3.6& \textrm{if } x_i=1 \\ 
                                          6.8 &\textrm{if } x_i=3 \end{array} \right. ,
\]
and finally $Y_i \sim N(\mu_i , \sigma^2)$ with $\sigma^2=1$.  
\begin{enumerate}[(a)]
{\item Simulate a single realization from the data-generating mechanism described above (with $n=20$)
and present the data in a scatterplot, with the $Y_i$ on the vertical axis and the $x_i$ on the horizontal axis.  Fit a simple linear regression model of the form
\[
Y_i = \beta_0 + x_i \beta_1 + \epsilon_i
\]
by ordinary least squares and overlay the fitted regression line on your scatterplot.  Comment on the approporiateness of fitting a linear model to these data.}

\begin{description}
\item[Solution:] See Figure \ref{fig:p1_ols}. 
  
  \begin{figure}
    \centering
    \includegraphics{p1_ols.pdf}
    \caption{Ordinary least squares for data according to the model in Problem 1.}
    \label{fig:p1_ols}
  \end{figure}

  The line isn't a very good fit. It overestimates when $x = 0$ and
  underestimates when $x = 1$. This isn't surprising since the linear model is
  incorrect.
  
\end{description}
{\item Let $\hat{\beta}_1$ be the estimated slope parameter from fitting the model above, and define $\beta_1$ to be the quantity for which $\hat{\beta}_1$ is consistent, in the limit as $n \rightarrow \infty$.  Using the result on Slide 1.18 of the course lecture notes, calculate $\beta_1$ exactly.}

\begin{description}
\item[Solution:] $\beta_0 = 1$ and $\boxed{\beta_1 = 2.}$
  
  Let $\mathbb{P}$ be the probability measure over $x$ and $y$. Let
  $\phi\left(x \mid \mu, \sigma^2\right)$ be the density of the normal
  distribution with mean $\mu$ and variance $\sigma^2$. Consider the integral,
  \begin{align*}
    L\left(\gamma\right)
    &= \int \left(y - \left(\gamma_0 + \gamma_1x\right)\right)^2\,\operatorname{d}\mathbb{P}\left(x, y\right) \\
    &= \frac{1}{3}\int_{-\infty}^\infty\left(y - \gamma_0\right)^2\phi\left(y\mid 0.6, 1\right)\,\operatorname{d}y + \\
    &~~~~\frac{1}{3}\int_{-\infty}^\infty\left(y - \gamma_0 - \gamma_1\right)^2\phi\left(y\mid 3.6, 1\right)\,\operatorname{d}y + \\
    &~~~~\frac{1}{3}\int_{-\infty}^\infty\left(y - \gamma_0 - 3\gamma_1\right)^2\phi\left(y\mid 6.8, 1\right)\,\operatorname{d}y \\
    &= \frac{1}{3}\left(
      62.56 - 22\gamma_0 - 48\gamma_1 + 3\gamma_0^2 + 10\gamma_1^2 + 8\gamma_0\gamma_1
      \right).      
  \end{align*}
  If we let $\beta = \argmin_\gamma L\left(\gamma\right)$. Taking the
  derviative, we setting
  $\frac{\partial L }{\partial \gamma}\left(\beta\right) = 0$ to obtain
  $\beta_0 = 1$ and $\beta_1 = 2$.
\end{description}

{\item Download White's seminal paper from 1980 on sandwich estimation from the course website .  Read (at least) the first few pages and explain how the value of $\beta_1$ you derived in part (b) also follows
  from Lemma 1 of the paper.}

\begin{description}
\item[Solution:] In fitting the model, we are assuming that the linear model is
  correct and Assumption 1 of the paper holds.

  Assumption 2 is satisfied so since $\mathbb{E}\left[\epsilon_i^2\right] = 1$
  for all $i$ and the expected covariance matrix =
  $\bar{M}_n = n^{-1}\sum_{i=1}^n\mathbb{E}\left[X_i^2\right]$ is just a matrix
  $1 \times 1$ with a single positive entry, so it is certainly nonsingular is
  with a positive determinant.

  Thus, Lemma 1 tells us that $\hat{\beta} \xrightarrow {\mathrm{a.s.}} \beta$,
  where $\hat{\beta} = \left(X^\intercal X\right)^{-1} X^\intercal Y$.

  Since $\hat{\beta}$ is also the maximum likelihood estimate which minimizes
  mean squared error, it converges almost surely to the value of $\beta$ that
  minimizes expected mean squared error.
\end{description}

{\item For finite $n$, we cannot discuss properties of the expectation of $\hat{\beta}_1$, at least not in the conventional sense.  Explain why not.}

\begin{description}
\item[Solution:] Note that
  $\hat{\beta} = \left(X^\intercal X\right)^{-1} X^\intercal Y$.  

  If the linear model were correct, that is, $Y = X\beta + \epsilon$, taking the
  expection would give
  \begin{equation}
    \mathbb{E}\left[\hat{\beta}\right]
    = \beta + \mathbb{E}\left[
      \left( X^\intercal X \right)^{-1} X^\intercal \epsilon
    \right] = \beta.
  \end{equation}

  However, This calculation is incorrect when $Y$ depends on $X$ in a nonlinear
  way as with these data.
\end{description}

\end{enumerate}
For the remainder of this problem, condition on realizations of the data for which there exist $i$ and $i^\prime$ such that $x_i \neq x_{i^\prime}$.
\begin{enumerate}[(a)] \addtocounter{enumi}{4}
{\item Conduct a simulation study to assess the bias of $\hat{\beta}_1$ for estimating $\beta_1$.  Plot the estimated bias as a function of $n$ (for $n=2,4,6,\ldots,28,30,35,\ldots,95,100$), and overlay the standard error of $\hat{\beta}_1$ as a function of $n$.  Note that for this problem you should estimate the standard error of $\hat{\beta}_1$ based on the 
  standard deviation of $\hat{\beta}_1$ across simulated realizations of the data, not using any further exact or asymptotic calculations.}

\begin{description}
\item[Solution:] See Table \ref{tab:p1_simulation_results} for the slope
  estimates $\hat{\beta}_1$ and standard error estimates $\hat{\sigma}$ for
  different values of $n$.

  \begin{table}
    \centering
    \input{p1_simulation_results.tex}
    \caption{The estimate for the slope ($\hat{\beta}_1$) and standard error for
      the estimate ($\hat{\sigma}$) were each calculated with $10^6$ trials. The
      last column $\mathbb{E}\left[\hat{\beta}_1\right]$ was calculated
      numerically by enumerating over the possible draws of $X$, which is detailed in Part (g).}
    \label{tab:p1_simulation_results}
  \end{table}

  \begin{figure}
    \centering
    \includegraphics{p1_simulation_results.pdf}
    \caption{Data from Table \ref{tab:p1_simulation_results} along with
      asymptotic slope ($\beta_1$), the bias ($\hat{\beta}_1 - \beta_1$), and
      variance ($\hat{\sigma}^2$).}
    \label{fig:p1_simulation_results}
  \end{figure}

  These data are plotted in Figure \ref{fig:p1_simulation_results}.
\end{description}

{\item Comment on your findings in part (e).  Is the bias in estimating $\beta_1$ surprising?  Why or why not?  How concerned should one be about this bias? }

\begin{description}
\item[Solution:] The bias is not suprising. As discussed in Part (d), the usual
  method of proving that the estimate is unbiased does not work since the true
  model is not linear.

  The bias decreases quickly with increasing $n$, so it is not too concerning
  with reasonable sample sizes. Moreover, the standard error is larger than the
  bias, so in the bias-variance decomposition,
  \begin{align*}
    \mathbb{E}\left[
      \left(\hat{\beta}_1 - \beta_1\right)^2
    \right]
    &=
      \mathbb{E}\left[
      \left(\hat{\beta}_1 - \beta_1\right)^2
      \right] =
    \mathbb{E}\left[\hat{\beta}_1^2\right] - 2\mathbb{E}\left[\hat{\beta}_1\right]\beta_1 + \beta_1^2  \\
    &= \operatorname{var}\left(\hat{\beta}_1\right) + \mathbb{E}\left[
      \hat{\beta}_1
      \right]^2  - 2\mathbb{E}\left[\hat{\beta}_1\right]\beta_1 + \beta_1^2  \\
    &= \left(\mathbb{E}\left[\hat{\beta}_1\right] - \beta_1\right)^2 + \operatorname{var}\left(\hat{\beta}_1\right) \\
    &= \left[\operatorname{Bias}\left(\hat{\beta}_1\right)\right]^2 + \operatorname{var}\left(\hat{\beta}_1\right),
  \end{align*}
  the variance term which is the standard error squared will dominate. Indeed,
  according to Figure \ref{fig:p1_simulation_results}, the variance is much
  larger than the bias when $n$ is small. When $n$ is large, both the bias and
  variance are small, so neither is too concerning.
\end{description}

{\item It is possible to derive exact values for the biases estimated in part (d) by enumerating all possible draws of $x_1, \ldots, x_n$.  Do this calculation for $n=2,4,6,8$.  Report your numerical findings and overlay them on your plot from part (e).   Show how you can do the calculation entirely by hand for $n=2$ (you may use a computer for matrix algebra).  Although it is highly advisable to use a computer for this calculation for $n>2$, simulation results are not sufficient.  Your calculation of the bias must be exactly correct up to the computer's floating point precision.}

\begin{description}
\item[Solution:] $\mathbb{E}\left[\hat{\beta}\right]$ can be efficiently
  calculated by noticing that
  \begin{equation}
    \mathbb{E}\left[\hat{\beta} \mid X \right] =
    \mathbb{E}\left[\left(X^\intercal X\right)^{-1}X^\intercal Y
      \mid X
    \right]
    = \left(X^\intercal X\right)^{-1}X^\intercal \mu.
  \end{equation}  
  is the same for different permuations of $X$.

  Assume that each $X_{(j_0,j_1,j_3)} \in \mathbb{R}^n$ is a vector of $j_0$
  $0$s, $j_1$ $1$s, and $j_3$ $3$s, where $j_0 + j_1 + j_3 = n$ and
  $j_0,j_1,j_3 \in \left\{0,1,\ldots, n - 1\right\}$. The triplets $\left(j_0, j_1, j_3\right)$
  can be seen as coming from the multinomial distribution, so
  \begin{equation}
    \mathbb{P}\left(\left(j_0, j_1, j_3\right)\right) = \frac{n!}{j_0!j_1!j_3!}\left(\frac{1}{3}\right)^n.
  \end{equation}

  Therefore, we have that
  \begin{align*}
    \mathbb{E}\left[\hat{\beta}\right]
    &= \mathbb{E}\left[\mathbb{E}\left[
      \hat{\beta}
      \mid X
      \right]\right] \\
    &= \sum_{\left\{
    \left(j_0, j_1, j_3\right) \in \left\{0,1,\ldots, n - 1\right\}^3
      \mid j_0 + j_1 + j_3 = n\right\}}
      \mathbb{P}\left(\left(j_0, j_1, j_3 = n\right)\right)
      \left(X^\intercal X\right)^{-1}X^\intercal \mu \\
    &= \sum_{\left\{
    \left(j_0, j_1, j_3\right) \in \left\{0,1,\ldots, n - 1\right\}^3
      \mid j_0 + j_1 + j_3 = n\right\}}
      \frac{n!}{j_0!j_1!j_3!}\left(\frac{1}{3}\right)^n
      \left(X^\intercal X\right)^{-1}X^\intercal \mu.
  \end{align*}

  \begin{figure}
    \centering
    \includegraphics{p1_bias_comparison}
    \caption{Comparison of the simulated bias from Part (e) against the exact
      calculation in Part (g).}
    \label{fig:p1_bias_comparison}
  \end{figure}
  
  The results of this calculation can be seen in the last column of Table
  \ref{tab:p1_simulation_results}. The biases from the simulation are plotted
  against the exact calculation in Figure \ref{fig:p1_bias_comparison}.

  Both the table and figure show that the simulation and calulations
  agree. There appears to be more noise with larger $n$, however.
\end{description}
\end{enumerate}



\section*{Problem 2: Average slope interpretation of generalized least squares (5 points)}
{\em Consider simple linear regression with a
single covariate and an intercept, such that we are fitting
\[
E(Y_i)=\beta_0 + \beta_1 x_i
\]
with scalar observations $Y_1,\ldots,Y_n$. As we saw on slide 1.18, the OLS estimator of $\beta_1$ can
be written as a weighted average of the pairwise slopes $(Y_i-Y_j)/(x_i-x_j)$.
This is called the ``average slope'' representation.
}
\begin{enumerate}[(a)]
{\em \item Suppose that $\bSigma$, the covariance of $\bY$, is a diagonal matrix with known entries $\sigma_1^2,\ldots,\sigma_n^2$.
  Derive the weights for an ``average slope'' representation for the optimal generalized least squares (GLS) estimate of $\beta_1$ (see exercise 8.1 in the Wakefield text if you are not sure of the form of the optimal GLS estimate).  A formal proof is optional, but you should provide a convincing justification for the specific form of your answer.  HINT: One way to approach this problem is to think about what happens when the $\sigma^2_i$ are all rational numbers, i.e., they can be expressed as fractions.}

\begin{description}
\item[Solution:] The likelihood function is
  \begin{equation}
    L_{X,y,\Sigma}\left(\beta\right) = f_\beta\left(y\right)
    = \frac{1}{\sqrt{\det\left(2\pi\Sigma\right)}}\exp\left(
      -\frac{1}{2}\left(y - X\beta\right)^{\intercal}\Sigma^{-1}\left(y - X\beta\right)
    \right).
  \end{equation}
  Maximizing this likelihood by taking the $\log$ and derivative, we estimate
  $\beta$ to obtain the maximum likelihood estimate
  $\hat{\beta} =
  \left(X^\intercal\Sigma^{-1}X\right)^{-1}X^\intercal\Sigma^{-1}y$.

  Expanding this out, we find that
  \begin{equation*}
    \hat{\beta} =
    \left.
      \begin{pmatrix}
        \displaystyle
      \left(\sum_{i=1}^n\frac{x_i^2}{\sigma_i^2}\right)
      \left(\sum_{i=1}^n\frac{y_i}{\sigma_i^2}\right) -
      \left(\sum_{i=1}^n\frac{x_i}{\sigma_i^2}\right)
      \left(\sum_{i=1}^n\frac{x_iy_i}{\sigma_i^2}\right) \\
      \displaystyle
      \left(\sum_{i=1}^n\frac{1}{\sigma_i^2}\right)\left(\sum_{i=1}^n\frac{x_iy_i}{\sigma_i^2}\right) -
      \left(\sum_{i=1}^n\frac{x_i}{\sigma_i^2}\right)
      \left(\sum_{i=1}^n\frac{y_i}{\sigma_i^2}\right)      
    \end{pmatrix}
    \middle/\left(
    \left(\sum_{i=1}^n\frac{1}{\sigma_i^2}\right)
      \left(\sum_{i=1}^n\frac{x_i^2}{\sigma_i^2}\right) -
      \left(\sum_{i=1}^n \frac{x_i}{\sigma_i^2}\right)^2
    \right)
  \right..  
\end{equation*}

Let
$\bar{x} = \left.\sum_{i=1}^n \frac{1}{\sigma_i^2}x_i\middle/\sum_{i=1}^n
  \frac{1}{\sigma_i^2}\right.$ denote the weighted mean. We can rewrite our
estimate for the slope
\begin{equation*}
  \hat{\beta}_1 = \frac{\sum_{i=1}^n\frac{1}{\sigma_i^2}\left(x_i - \bar{x}\right)\left(y_i - \bar{y}\right)}
  {\sum_{i=1}^n\frac{1}{\sigma_i^2}\left(x_i - \bar{x}\right)^2}
  =
  \frac
  {
    \sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}\sum_{k=1}^n\frac{1}{\sigma_k^2}
    \left(x_i - x_j\right)\left(y_i - y_k\right)
  }
  {
    \sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}\sum_{k=1}^n\frac{1}{\sigma_k^2}
    \left(x_i - x_j\right)\left(x_i - x_k\right)
  }.
\end{equation*}

Now with some algebra,
\begin{align*}
  \sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}\sum_{k=1}^n\frac{1}{\sigma_k^2}
  \left(x_i - x_j\right)\left(y_i - y_k\right)
  &= \sum_{j=1}^n\frac{1}{\sigma_j^2}\sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{k=1}^n\frac{1}{\sigma_k^2}
    \left(x_i - x_j\right)\left(y_i - y_k\right) \\
  &= \left(\sum_{k=1}^n\frac{1}{\sigma_k^2}\right)
    \sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}
    \left(x_i - x_j\right)y_i \\
  &= \frac{1}{2}\left(\sum_{k=1}^n\frac{1}{\sigma_k^2}\right)
    \sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}
    \left(x_i - x_j\right)\left(y_i - y_j\right),
\end{align*}
where the last two steps take advantage of the symmetry of the $i$ and $j$
indices. Using this result, we have can write our slope estimate
\begin{align*}
  \hat{\beta}_1
  &=
    \frac{\sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}
    \left(x_i - x_j\right)\left(y_i - y_j\right)}
  {\sum_{i=1}^n\frac{1}{\sigma_i^2}\sum_{j=1}^n\frac{1}{\sigma_j^2}
    \left(x_i - x_j\right)^2} \\
  &= \boxed{\left.
    \sum_{i=1}^n\sum_{\substack{j=1 \\ x_i \neq x_j}}^n \left(\frac{\left(x_i - x_j\right)^2}{\sigma_i^2\sigma_j^2}\right)
    \left(\frac{y_i - y_j}{x_i - x_j}\right)
    \middle/
    \sum_{i=1}^n\sum_{j=1}^n \frac{\left(x_i - x_j\right)^2}{\sigma_i^2\sigma_j^2}
    \right..}
\end{align*}

Thus, the unnormalized weight for the slope between $i$ and $j$ is
$\boxed{\displaystyle \frac{\left(x_i - x_j\right)^2}{\sigma_i^2\sigma_j^2}.}$
The normalized weights are
$\boxed{\displaystyle w_{ij} = \left.\frac{\left(x_i - x_j\right)^2}{\sigma_i^2\sigma_j^2}
    \middle/\sum_{i=1}^n\sum_{j=1}^n \frac{\left(x_i - x_j\right)^2}{\sigma_i^2\sigma_j^2}\right..}$

Intuitively, this make sense. As $\sigma_i^2$ and $\sigma_j^2$ increase, the
weight decreases since our estimated slope is noisier. On the other hand, as
$(x_i - x_j)^2$ increases the weight increases because the slope estimate
becomes more robust to noise since the effect size is larger. To see this
mathematically, note that
$\frac{y_i - y_j}{x_i - x_j} = \beta_1 + \frac{\sigma_i\epsilon_i -
  \sigma_j\epsilon_j}{x_i - x_j}$, where
$\epsilon_i \sim \mathcal{N}\left(0,1\right)$. The error term increases in
magnitude as a function of $\sigma_i^2$ and $\sigma_j^2$ and decreases in
magnitude as a function of $(x_i - x_j)^2$.
\end{description}
{\item Now consider what the weights would be in an ``average slope''
representation for the optimal GLS estimate of $\beta_1$, for a general covariance matrix $\bSigma$.  Give some properties of the weights that you would expect to hold.  In answering this question, it might be helpful to think about how the weights would change as you vary individual entries in $\bSigma$.
Also discuss any situations where you would expect the weight for a particular pair of observations to 
be infinite or zero.  Note that you are {\bf not} asked to derive an average slope representation for GLS.}

\begin{description}
\item[Solution:] Let $\mathbf{1}$ be a vector a in $\mathbb{R}^n$ of all
  $1$s. Denote the weighted mean,
  $\bar{x} =
  \left.\mathbf{1}^\intercal\Sigma^{-1}x\middle/\mathbf{1}^\intercal\Sigma^{-1}\mathbf{1}\right.$
  Denote the centered vector, $\tilde{x} = x - \bar{x}\mathbf{1}$.

  Then, we have that
  \begin{equation}
    \hat{\beta}_1 = \frac{\tilde{x}^\intercal\Sigma^{-1}\tilde{y}}{\tilde{x}^\intercal\Sigma^{-1}\tilde{x}}
    = \left.
      \sum_{i=1}^n\sum_{j=1}^n \Sigma^{-1}_{ij}\tilde{x}_i\tilde{y}_j
      \middle/
      \sum_{i=1}^n\sum_{j=1}^n \Sigma^{-1}_{ij}\tilde{x}_i\tilde{x}_j.
      \right.
    \end{equation}

    Let $\Delta \in \mathbb{R}^{n \times n}$ be the skew-symmetric matrix where
    $\Delta_{ij} = x_i - x_j$. Let $\Sigma_i$ denote the $i$th row or column of
    the covariance matrix (the matrix is symmetric). We can rewrite our slope
    estimate as
    \begin{equation*}
      \hat{\beta}_1 = \left.\sum_{i=1}^n\sum_{\substack{j=1 \\ x_i \neq x_j}}^n
      \frac{y_i - y_j}{x_i - x_j}\left(x_i - x_j\right)\operatorname{tr}\left(
      \Delta\Sigma_i^{-1}\left(\Sigma_j^{-1}\right)^\intercal
    \right)
    \middle/
    \sum_{i=1}^n\sum_{j=1}^n\left(x_i - x_j\right)\operatorname{tr}\left(
      \Delta\Sigma_i^{-1}\left(\Sigma_j^{-1}\right)^\intercal
    \right),\right.
    \end{equation*}
    where $\operatorname{tr}$ is the trace operator. Since
    \begin{align}
      -\left(x_i - x_j\right)\operatorname{tr}\left(
        \Delta\Sigma_i^{-1}\left(\Sigma_j^{-1}\right)^\intercal
      \right) 
      &= \left(x_i - x_j\right)\sum_{k=1}^n\sum_{l=1}^n
        \left(x_k - x_l\right)\Sigma_{ik}^{-1}\Sigma_{jl}^{-1} \nonumber \\
      &= \left(x_i - x_j\right)\sum_{k=1}^n\sum_{l=k + 1}^n
        \left(x_k - x_l\right)\det\begin{pmatrix}
          \Sigma_{ik}^{-1} & \Sigma_{il}^{-1} \\
          \Sigma_{jk}^{-1} & \Sigma_{jl}^{-1}
        \end{pmatrix},
    \end{align}
    we can see the pairwise slopes are weighted according to the entries from
    the precision matrix. Moreover, the situation differs from the independent
    case in that the weight of each slope depends on all possible pairwise
    differences $x_j - x_l$, which intuitively makes sense since there may be
    some correlation with the other observations.

    Let $\tilde{\rho}_{ij}$ be the partial correlation coefficient between $Y_i$
    and $Y_j$ given the remaining $Y_k$. The partial correlation measures the
    correlation between observations controlling for cofounders. Entries of the
    precision matrix are the scaled partial correlations,
    $\Sigma_{ij}^{-1} = \tilde{\rho}_{ij}\sqrt{\Sigma_{ii}^{-1}\Sigma_{jj}^{-1}}$.

    With this identity, we can rewrite unnormalized weights for the slope
    estimate
    \begin{equation}
      -\left(x_i - x_j\right)\operatorname{tr}\left(
        \Delta\Sigma_i^{-1}\left(\Sigma_j^{-1}\right)^\intercal
      \right) = \left(x_i - x_j\right)
      \Sigma_{ii}^{-1}\Sigma_{jj}^{-1}
      \sum_{k=1}^n\sum_{l=k + 1}^n
      \left(x_k - x_l\right)\det\begin{pmatrix}
        \tilde{\rho}_{ik} & \tilde{\rho}_{il} \\
        \tilde{\rho}_{jk} & \tilde{\rho}_{jl}
        \end{pmatrix},
        \label{eqn:p2_slope_correlation}
    \end{equation}
    
    Equation \ref{eqn:p2_slope_correlation} gives us insight into how the
    covariance matrix $\Sigma$ affects the weights for the pairwise slope
    between observations $i$ and $j$.

    As before, large differences $x_i - x_j$ increase the magnitude of the
    weight, and large variances, manifested as small entries of
    $\Sigma_{ii}^{-1}$ and $\Sigma_{jj}^{-1}$, decrease the magnitude of the
    weight. Now, the differences $x_j - x_l$ also affect the weight in a similar
    manner to $x_i - x_j$. The partial correlation coefficients govern the
    degree of this effect. If there is no partial correlation, that is,
    $\tilde{\rho}_{ik} = 0$ and $\tilde{\rho}_{il} = 0$ or
    $\tilde{\rho}_{jk} = 0$ and $\tilde{\rho}_{jl} = 0$, then the term involving
    $x_{k} - x_{l}$ is zeroed out. Of course, when the observations are
    independent only the terms with $\left(k,l\right) = \left(i,j\right)$ or
    $\left(k,l\right) = \left(j,i\right)$ are nonzero, and we recover the same
    weights as the previous part.
  \end{description}
\end{enumerate} 


\section*{Problem 3: Cross-sectional/longitudinal effects, partitioning the exposure, mean model misspecification (10 points)}
{\em Consider a made-up computer literacy trial similar to the one discussed in lecture (slides 1.35--1.48).  There are $n$ subjects, indexed by $i=1,\ldots,n$, each one of which is observed at
$m$ followup times, indexed by $j=1,\ldots,m$.  Let $Y_{ij}$ be the literacy score of subject $i$ at follow-up time $j$, at which time the subject's age is $x_{ij}$.  Suppose the design is fixed, meaning that the $x_{ij}$ are all
deterministic and known in advance, and assume the true data-generating
mechanism can be written
\begin{equation}
\label{eqn:p3_eq1}
Y_{ij}=f(x_{i1})+\beta_L(x_{ij}-x_{i1})+\epsilon_{ij}
\end{equation}
with i.i.d. $\epsilon_{ij}\sim N(0,\sigma^2)$.  The unknown function $f(\cdot)$ 
represents a potentially nonlinear cohort effect and $\beta_L$ is the coefficient for the 
linear longitudinal effect.
Suppose we try to estimate $\beta_L$ using 
the exposure partioning method proposed on slide 1.44
(see, also, Diggle et al. page 16), based on the linear regression estimating equations
with the mean model
\begin{equation}
\label{eqn:p3_eq2}
E(Y_{ij})=\beta_0 + \beta_C x_{i1} + \beta_L (x_{ij}-x_{i1}).
\end{equation}}
\begin{enumerate}[(a)]
{\em \item Give a sufficient condition on the data-generating mechanism for $Y_{ij}$ in equation~(\ref{eqn:p3_eq1}) to guarantee
  unbiased estimation of $\beta_L$ and valid model-based standard errors.}

\begin{description}
\item[Solution:] Let $\tilde{X}$ be the matrix with $1$s in the first columns,
  $x_{i1}$ (repeated as necessary) in the second column and $x_{ij} - x_{i1}$ in
  the third column. Let $Y$ be a vector of the corresponding literacy scores. We
  can estimate $\beta_L$ with the least squares estimator
  \begin{equation}
    \hat{\beta} =
    \begin{pmatrix}
      \hat{\beta}_0 \\
      \hat{\beta}_C \\
      \hat{\beta}_L
    \end{pmatrix}
    =
    \left(\tilde{X}^\intercal\tilde{X}\right)^{-1}\tilde{X}^\intercal Y.
  \end{equation}
  If we substitute Equation \ref{eqn:p3_eq1} for $Y$,
  $\mathbb{E}\left[\hat{\beta}\right] = \beta$ if $f$ is a linear function, that
  is, $f\left(x_{i1}\right) = \beta_0x_{i1}$ for some constant $\beta_0$. 

  Given $f$ is linear, the covariance matrix for our estimator is
  \begin{equation}
    \operatorname{var}\left(\hat{\beta}\right)
    =
    \sigma^2\left(\tilde{X}^\intercal \tilde{X}\right)^{-1}.
  \end{equation}
\end{description}

{\em \item Give a sufficient condition on the choice of follow-up times in the study design to guarantee
unbiased estimation of $\beta_L$, even if the condition in part (a) is violated.  Thinking about
when confounding bias is a problem will be helpful here.}

\begin{description}
\item[Solution:] To control for the cofounding factors, we can use
  randomization. In particular, if follow-up times are selected independently of
  the subjects' ages, any systematic bias from incorrectly predicting base
  literacy rates will be eliminated.

  To see this, note that if the covariates are independent
  $\mathbb{E}\left[\tilde{X}^\intercal \tilde{X}\right]$ is a diagonal matrix
  (without loss of generality assume $\tilde{X}$ is centered), so the
  coefficients are estimated independently of each other.
\end{description}

{\em \item Download the modified made-up literacy data from the course web site (you can find these data with the homework files, not in the general dataset section), and generate
one or more exploratory plots.  Describe how you can graphically see evidence that neither of
the above conditions is satisfied.}

\begin{description}
\item[Solution:] See the plotted data in Figure \ref{fig:p3_data}.

  \begin{figure}
    \centering
    \includegraphics{p3_data.pdf}
    \caption{In the computer literacy data, each subject has 3 observations
      taken at similar follow-up periods.}
    \label{fig:p3_data}
  \end{figure}
  Looking at the red points, one sees that $f$ is clearly not linear, so the
  assumption in Part (a) is violated. Each subject was followed up with
  twice. $x_{i3} - x_{i2}$ appears to be similar to $x_{i2} - x_{i1}$. For older
  subjects, the follow up periods appear to be shorter. Clearly, the follow up
  times were not selected independently of the subjects' ages, so the assumption
  Part (b) is violated.
\end{description}

{\em \item Propose an alternative regression model that can still be used to unbiasedly estimate $\beta_L$ and give valid model-based standard errors.  Fit this model to the downloaded data using the $\texttt{lm()}$ function in $\texttt{R}$ and report your findings (point estimate and standard error).
  Compare to what you obtain by fitting model~(\ref{eqn:p3_eq2}) to the data.}

\begin{description}
\item[Solution:] We can fit a model directly to the difference
  $Y_{ij} - Y_{i1} = \beta_L\left(x_{ij}- x_{i1}\right) + \left(\epsilon_{ij} -
    \epsilon_{i1}\right)$, $j \neq 1$. In this way, we discard one-third of our
  observations. Let $\tilde{X}_3$ be the third column of $\tilde{X}$ and let
  $\tilde{Y}$ be the vector of $Y_{ij}- Y_{i1}$. Assume that we have discarded
  the $j = 1$ entries. Then, we can our new model can estimated
  $\hat{\beta}_L =
  \left(\tilde{X}_3^\intercal\tilde{X}_3\right)^{-1}\tilde{X}_3^\intercal\tilde{Y}$.
  Since the expected residual is $0$ this estimator is always unbiased.

  Fitting this model to the data, we find that
  $\boxed{\hat{\beta}_L = 1.02185154}$ with corresponding standard error
  $\boxed{\sqrt{\hat{\operatorname{var}}\left(\hat{\beta}_L\right)} =
    0.032418976.}$

  If we fit the model in Equation \ref{eqn:p3_eq2}, we have
  $\boxed{\hat{\beta}_L = 1.24721578}$ with corresponding standard error
  $\boxed{\sqrt{\hat{\operatorname{var}}\left(\hat{\beta}_L\right)} = 1.06297.}$
  Thus, the misspecified model leads to a much higher standard error.
\end{description}

{\em \item Design and conduct a simulation study to illustrate the following 
\begin{enumerate}
\item Model~(\ref{eqn:p3_eq2}) can fail to give unbiased estimates of $\beta_L$ without at least one of the additional conditions from parts (a) and (b).
\item Model~(\ref{eqn:p3_eq2}) does give unbiased estimates if either of the additional conditions from parts (a) and (b) is satisfied.
\item The model you propose in part (d) works, regardless.
\end{enumerate}
You will need to consider a few choice of $f(\cdot)$ and study designs $\{x_{ij}\}$
to illustrate points (i) through (iii).  Note that in earlier parts of the problem, you should have 
argued theoretically that each of these points is true; the purpose of the simulation study is to verify
and illustrate your conclusions. }

\begin{description}
\item[Solution:] I fixed $\beta_L = 2$, $\sigma^2 = 1$, and use 64
  subjects. If I specified 2 follow-up times for each subject and let
  $f\left(x\right) = 200*\exp\left(-x\right)$, both assumptions are
  violated. Indeed, fitting the model in Equation \ref{eqn:p3_eq2} led to a
  biased estimate of $2.16$. The alternative model from the previous section
  handle this case fine and produced an unbiased estimate.

  Next, I enforced the condition in Part (a) and made $f$ linear,
  $f\left(x\right) = 200 - 12x$. Both the model in Equation \ref{eqn:p3_eq2} and
  the alternative model result in unbiased estimates.

  When it's enforced that the follow up times are independent of the base age,
  again both models produce unbiased estimates even if the $f$ is a nonlinear
  function like $f\left(x\right) = 200*\exp\left(-x\right)$.

  Code for all these simulations is found in the Appendix.
\end{description}

{\em \item In your simulation study, what do you notice about the standard error estimates from the model in equation~(\ref{eqn:p3_eq2}) when the condition from part (b) is satisfied but
  the condition from part (a) is not satisfied?  Do sandwich standard errors fix the problem?  }

\begin{description}
\item[Solution:] With the usual linear regression estimate for the variance of
  $\hat{\beta}_L$, the standard errors are overestimated. My simulations show
  that the 95\% confidence interval actually covered the true mean 98.1\% of the
  time.

  Sandwich estimation seems to help a little, but the situation is the same with
  the same: the 95\% confidence interval with the sandwich standard errors
  covers the true mean 97.75\% of the time, so it also overestimates the
  standard error.
\end{description}

{\em \item Explain, theoretically, what is going wrong in part (f) of this problem.
This involves some fairly careful thinking about misspecified mean models and what is required
for sandwich estimation to be valid.}

\begin{description}
\item[Solution:] Sandwich estimation protects against misspecification of the
  variance by using empirical estimates. The \emph{bread} part of the sandwich
  estimator in the case of ordinary least squares is $X^\intercal\hat{\Sigma}X$,
  where $\hat{\Sigma}_{ii}$ is diagonal, and
  $\hat{\Sigma}_{ii} = \left(Y_i - X\hat{\beta}\right)^2$. Thus, the diagonal
  contains empirical residual estimates. Those estimates depend on our mean
  model, which in this case is incorrect, biased, and inconsistent since the $f$
  is nonlinear. Thus, the results of the sandwich estimator will also be biased
  and inconsistent.
\end{description}

\end{enumerate} 

\section*{Appendix}

Code for fitting models and generating plots is attached.

\includepdf[pages=-]{p1_simulation.pdf}
\includepdf[pages=-]{p3_regression.pdf}

\end{document}